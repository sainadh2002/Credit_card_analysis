# -*- coding: utf-8 -*-
"""credit_card.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RV7y4NzMl4Vwjanaa3dBFKVTntPF1RVS
"""

#import libraries
import numpy as np
import pandas as pd

#Import the dataset
df = pd.read_csv('/content/creditcard.csv')
df

#shape of the data
df.shape

"""#FINDING MISSING OR NULL VALUES

"""

# information about the data
df.info()

#to know the null values are present in the data or not
df.isnull()

#to know the count of null values in each column
df.isnull().sum()

#total count of null values
df.isnull().sum().sum()

"""#HANDLE THE MISSING VALUES"""

#fill the missing values in numerical_coloumns with aproximate number using KNN
from sklearn.impute import KNNImputer
impute=KNNImputer()
for i in df.select_dtypes(include='number').columns:
  df[i]=impute.fit_transform(df[[i]])

#check the null values after the treatment of numerical coloums
df.isnull().sum()

#handle the remaining text columns with fillna(ffill) -> forward_fill

df['Good_Bad'] = df['Good_Bad'].ffill()
df['Rented_Own House'] = df['Rented_Own House'].ffill()
df['Occupation'] = df['Occupation'].ffill()

df.isnull().sum()

#handle the remaining text columns with fillna(ffill) -> forward_fill
df['Gender'] = df['Gender'].ffill()
df['Education'] = df['Education'].ffill()
df.isnull().sum()

#handle the reaminig to coloums using fillna(mode)method

df['Number Of Dependents'] = df['Number Of Dependents'].fillna(df['Number Of Dependents'].mode()[0])
df['Region'] = df['Region'].fillna(df['Region'].mode()[0])
df.isnull().sum()

#after cleaning the data once check the data
df

#to remove repeated column in data(dropna)

df1 = df.drop(['Monthly Income.1'],axis=1)
df1

#to know the any duplicates are present in data
df1.duplicated().sum()

#to remove the duolicate values (dropna)
df2 = df1.drop_duplicates()
df2

#after data handling shape
df2.shape

#cross check the null values
df2.isnull().sum()

#data after cleaning
df2

"""#FINDING OUTLIERS IN DATA"""

#import required libraries
import seaborn as sns
import matplotlib.pyplot as plt

#to see coloumns in data
df2.columns

#to see outliers using boxplot
import warnings
warnings.filterwarnings("ignore")
for i in df2.select_dtypes(include="number").columns:
  sns.boxplot(data=df,x=i)
  plt.show()

"""#HANDLE THE OUTLIERS USING KNN"""

#detecting outliers in a box plot gives
#whisker gives lower and upper values for a given numerical column

def whisker(col):
    q1, q3 = np.percentile(col, [25, 75])
    iqr = q3 - q1
    lw = q1 - 1.5 * iqr
    uw = q3 + 1.5 * iqr
    return lw, uw

df2.columns

#whisker values for example
whisker(df2['NPA Status'])

whisker(df2['Revolving Utilization Of Unsecured Lines'])

#to handle outliers for numerical column
for i in ['NPA Status','Revolving Utilization Of Unsecured Lines','Monthly Income','Number Of Time 30-59 Days Past Due Not Worse',
          'Debt Ratio','Number Of Open Credit Lines And Loans','Number Of Times 90 Days Late','Number Real Estate Loans Or Lines',
          'Number Of Time 60-89Days Past Due Not Worse']:
       lw, uw = whisker(df2[i])
       df2[i] = np.where(df2[i] < lw, lw, df2[i])
       df2[i] = np.where(df2[i] > uw, uw, df2[i])

#after handle the outliers to showing the boxplot
for i in ['NPA Status','Revolving Utilization Of Unsecured Lines','Monthly Income','Number Of Time 30-59 Days Past Due Not Worse',
          'Debt Ratio','Number Of Open Credit Lines And Loans','Number Of Times 90 Days Late','Number Real Estate Loans Or Lines',
          'Number Of Time 60-89Days Past Due Not Worse']:
  sns.boxplot(df2[i])
  plt.show()

"""#CONVERT THE HANDLE DATA INTO CSV_FILE"""

df2.to_csv('credit_card.csv')

df2.shape

cf=pd.read_csv('/content/credit_card.csv')

cf.shape

#converted data
cf

cf.isnull().sum()

cf.describe()

"""#BELL_CURVE

"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
cf = np.random.normal(loc=50, scale=10, size=1000)

mean = np.mean(cf)
std_dev = np.std(cf)

x_axis = np.arange(start=mean - (3 * std_dev), stop=mean + (3 * std_dev), step=0.1)

plt_y = norm.pdf(x_axis, mean, std_dev)

plt.figure(figsize=(8, 6))
#plt.hist(data, bins=30, density=True, alpha=0.6, label='Data Histogram')
plt.plot(x_axis, plt_y, color='red', label='Bell Curve')
plt.xlabel('Value')
plt.ylabel('Probability Density')
plt.title('Bell Curve for the Data')
plt.legend()
plt.show()